"""
Common tools for performing an analysis collected into a single class
`Analysis` that can be subclassed by specific analyses.
"""


from __future__ import absolute_import, division

from collections import Mapping, OrderedDict, Sequence
from copy import deepcopy
from itertools import product
import sys
import time

import numpy as np
import scipy.optimize as optimize

from pisa import EPSILON, FTYPE, ureg, Q_
from pisa.core.map import Map, MapSet
from pisa.core.param import ParamSet
from pisa.utils.config_parser import parse_minimizer_config, PISAConfigParser
from pisa.utils.fileio import to_file
from pisa.utils.fisher_matrix import get_fisher_matrix
from pisa.utils.log import logging
from pisa.utils.minimization import set_minimizer_defaults, minimizer_x0_bounds,\
                                    validate_minimizer_settings,\
                                    display_minimizer_header, run_minimizer,\
                                    MINIMIZERS_USING_SYMM_GRAD,\
                                    LOCAL_MINIMIZERS_WITH_DEFAULTS
from pisa.utils.pull_method import calculate_pulls
from pisa.utils.stats import METRICS_TO_MAXIMIZE, it_got_better


__all__ = ['Analysis', 'apply_fit_settings', 'ANALYSIS_METHODS']

__author__ = 'J.L. Lanfranchi, P. Eller, S. Wren, T. Ehrhardt'

__license__ = '''Copyright (c) 2014-2018, The IceCube Collaboration

 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.'''

ANALYSIS_METHODS = ('minimize', 'scan', 'pull')
"""Allowed parameter fitting methods."""

# Define names that users can specify in configs such that the eval of those
# strings works.
numpy = np # pylint: disable=invalid-name
inf = np.inf # pylint: disable=invalid-name
units = ureg # pylint: disable=invalid-name

def t23_octant(fit_info):
    """Check that theta23 is in the first or second octant.

    Parameters
    ----------
    fit_info

    Returns
    -------
    octant_index : int

    Raises
    ------
    ValueError
        Raised if the theta23 value is not in first (`octant_index`=0) or
        second octant (`octant_index`=1)

    """
    valid_octant_indices = (0, 1)

    theta23 = fit_info['params'].theta23.value
    octant_index = int(
        ((theta23 % (360 * ureg.deg)) // (45 * ureg.deg)).magnitude
    )
    if octant_index not in valid_octant_indices:
        raise ValueError('Fitted theta23 value is not in the'
                         ' first or second octant.')
    return octant_index


def apply_fit_settings(fit_settings, free_params):
    """Validate fit settings (cf. `config_parser.parse_fit_config`) against
    a `DistributionMaker`'s set of free parameters. Ensure that params,
    ranges, test points, seeds etc. are compatible with free parameter specs.

    Returns a modified fit_settings dict.

    Parameters
    ----------
    fit_settings : dict
        Dictionary of fit settings as generated by
        `config_parser.parse_fit_config`

    free_params : ParamSet
        free parameters to which the fit settings are to be applied

    """
    processed_fit_settings = deepcopy(fit_settings)
    fit_methods = fit_settings.keys()
    # TODO: don't require all possible methods to be present
    # assert set(fit_methods) == set(ANALYSIS_METHODS)

    # wildcard can only occur once in fit_settings; all parameters not
    # specified will be treated by the method which has the wildcard
    wildcard = '*'

    method_with_wildcard = [
        fit_method for fit_method in fit_methods
        if wildcard in fit_settings[fit_method]['params']
    ]
    assert len(method_with_wildcard) <= 1

    params_with_fit_method = [
        pname for fit_method in fit_methods
        for pname in fit_settings[fit_method]['params'].keys()
        if pname != wildcard
    ]

    for pname in params_with_fit_method:
        # require to be a free parameter
        if not pname in free_params.names:
            raise ValueError(
                'Parameter "%s" present in fit settings but not among free'
                ' parameters "%s". Please ensure consistency.'
                % (pname, free_params.names)
            )

    # remaining
    params_remaining = [pname for pname in free_params.names
                        if pname not in params_with_fit_method]

    if method_with_wildcard:
        method_with_wildcard = method_with_wildcard[0]
        processed_fit_settings[method_with_wildcard]['params'].pop(wildcard)
        # these need the fit method settings defaults
        defaults = processed_fit_settings[method_with_wildcard].pop('defaults')
        processed_fit_settings[method_with_wildcard]['params'].update(
            {premain: defaults for premain in params_remaining}
        )
    elif params_remaining:
        raise ValueError(
            'Cannot tell how to fit the free parameters %s. Please provide'
            ' these in the fit settings or use the wildcard "%s".'
            % (params_remaining, wildcard)
        )

    # remove any 'default' entry (shouldn't be here in the first place)
    # for the other fit methods which don't make use of wildcard
    for fit_method in fit_methods:
        processed_fit_settings[fit_method].pop('defaults', None)

    # make the scan/pull values for each parameter
    # (can apply identical treatment)
    # These are constructed from a range and an nvalues integer, where
    # the range can be given as "nominal+/-<scale>*nominal.
    # TODO: compare fit ranges to free_params ranges, precedence?
    for fit_method in ['pull', 'scan']:
        new_values_d = {'params': [], 'values': []}
        if not fit_method in processed_fit_settings:
            processed_fit_settings[fit_method] = new_values_d
            continue
        for pname, sett_d in processed_fit_settings[fit_method]['params'].items():
            # keys correspond to individual parameter fit settings
            found_keys = set(sett_d.keys())
            # a certain combination of keys/options is allowed
            if fit_method == 'scan':
                valid_keys = [set(('nvalues', 'range')), set(('values',))]
            else:
                valid_keys = [set(('lin_range',))]
            # one of the combinations must be an exact hit
            if not found_keys in valid_keys:
                raise KeyError(
                    'Only recognised key (combinations) for %s method: %s. You'
                    ' tried to set %s for parameter "%s".'
                    % (fit_method, valid_keys, found_keys, pname)
                )
            # interpret and process the fields which we have depending on the
            # fit method in question
            if fit_method == 'scan':
                try:
                    nvals = int(sett_d['nvalues'])
                    prange = sett_d['range']
                except:
                    nvals, prange = None, None
                    values = sett_d['values']
            else:
                # pull method
                nvals = 2
                prange = sett_d['lin_range']

            # record the units of the target parameter and do consistency checks
            target_units = free_params[pname].units
            if prange is None:
                # this means we must have the case of scanning and scan values
                # being specified directly
                values = eval(values)
                if not isinstance(values, Q_):
                    raise TypeError(
                        'Please specify scan values for param "%s" with units'
                        ' (convertible to: "%s").' % (pname, target_units)
                    )
                try:
                    values.ito(target_units)
                except:
                    logging.error(
                        'The units ("%s") specified for parameter "%s" are not'
                        ' compatible with those ("%s") of the corresponding'
                        ' parameter in the `ParamSet` of free parameters.'
                        % (values.units, pname, target_units)
                    )
                    raise
            else:
                if fit_method == 'pull':
                    prange = eval(prange)
                    print len(prange)
                # need to convert from range and nvalues to linearly spaced
                # values themselves
                if isinstance(prange, basestring):
                    try:
                        # first detect the fraction of the nominal value which
                        # will correspond to one half of the range
                        scale_nom = float(
                            prange[prange.find('+/-') + 3:prange.find('*')]
                        )
                    except:
                        logging.error(
                            'Could not interpret range string "%s" for'
                            ' parameter "%s". Please specify as'
                            ' "nominal+/-<float>*nominal".'
                            % (prange, pname)
                        )
                        raise
                    nom = free_params[pname].nominal_value
                    half_width = scale_nom * nom
                    prange = [nom - half_width, nom + half_width]
                elif isinstance(prange, Q_):
                    #if not isinstance(prange, Sequence):
                    #    raise TypeError(
                    #        'Range specified for parameter "%s" is not'
                    #        ' a sequence but of "%s".' % (pname, type(prange))
                    #    )
                    if not len(prange) == 2:
                        raise ValueError(
                            'Range "%s" specified for parameter "%s" is not'
                            ' of length 2.' % (prange, pname)
                        )
                    try:
                        prange.ito(target_units)
                    except:
                        logging.error(
                            'The units ("%s") specified for parameter "%s" are'
                            ' not compatible with those ("%s") of the'
                            ' corresponding parameter in the `ParamSet` of free'
                            ' parameters.' % (prange.units, pname, target_units)
                        )
                        raise
                else:
                    raise TypeError(
                        'Range "%s" specified for parameter "%s" is of "%s" which'
                        ' is unhandled.' % (prange, pname, type(prange))
                    )
                values = np.linspace(prange[0], prange[1], nvals) * target_units

            new_values_d['params'].append(pname)
            new_values_d['values'].append(values)

        # overwrite entry with the new dict
        processed_fit_settings[fit_method] = new_values_d

    new_minimize_settings_d = {'global': None, 'local': None, 'params': []}
    if 'minimize' in processed_fit_settings:
        minimize_settings = processed_fit_settings['minimize']
        # all we do here for now is move 'global' and 'local' minimizer cfg
        # out from under one param to its own entry (assuming all minimizer cfg's
        # are identical) and make a simple list of params
        # TODO: could also allow for min. ranges to be specified in fit settings,
        # or for more complex things such as seeds
        for pname, sett_d in minimize_settings['params'].items():
            for opt in ['local', 'global']:
                # ensure necessary condition for this being parsed min. cfg
                assert isinstance(sett_d[opt], Mapping) or sett_d[opt] is None
                # also ensure all are identical
                if new_minimize_settings_d[opt]:
                    assert sett_d[opt] == new_minimize_settings_d[opt]
                new_minimize_settings_d[opt] = sett_d[opt]
            new_minimize_settings_d['params'].append(pname)

    processed_fit_settings['minimize'] = new_minimize_settings_d

    return processed_fit_settings


class Counter(object):
    """Simple counter object for use e.g. as a minimizer callback."""
    def __init__(self, i=0):
        self._count = i

    def __str__(self):
        return str(self._count)

    def __repr__(self):
        return str(self)

    def __iadd__(self, inc):
        self._count += inc
        return self

    def reset(self):
        """Reset counter"""
        self._count = 0

    @property
    def count(self):
        """int : Current count"""
        return self._count


class Analysis(object):
    """Major tools for performing "canonical" IceCube/DeepCore/PINGU analyses.

    * "Data" distribution creation (via passed `data_maker` object)
    * Asimov distribution creation (via passed `distribution_maker` object)
    * Minimizer Interface (via method `_minimizer_callable`)
        Interfaces to a minimizer for modifying the free parameters of the
        `distribution_maker` to fit its output (as closely as possible) to the
        data distribution is provided. See [minimizer_settings] for

    """
    def __init__(self):
        self._nit = 0
        self.counter = Counter()

    def optimize_discrete_selections(
            self, data_dist, hypo_maker, hypo_param_selections,
            extra_param_selections, metric, fit_settings=None, reset_free=True,
            check_octant=True, minimizer_settings=None, other_metrics=None,
            return_full_scan=False, blind=False, pprint=True
    ):
        # let someone pass just a single extra param selection
        # (which could just as well be part of the regular
        # hypo param selections), and then let hope enter our anguish
        if not isinstance(extra_param_selections, Sequence):
            extra_param_selections = [extra_param_selections]

        if not isinstance(hypo_param_selections, Sequence):
            hypo_param_selections = [hypo_param_selections]

        start_t = time.time()

        # here we store the (best) fit(s) for each discrete selection
        fit_infos = []
        fit_metric_vals = []
        fit_num_dists = []
        fit_times = []
        for extra_param_selection in extra_param_selections:
            if (extra_param_selection is not None and
                extra_param_selection in hypo_param_selections):
                raise ValueError('Your extra parameter selection "%s" has already '
                                 'been specified as one of the hypotheses but the '
                                 'fit has been requested to minimize over it. These '
                                 'are incompatible.' % extra_param_selection)
            # combine any previous param selection + the extra selection
            full_param_selections = hypo_param_selections
            full_param_selections.append(extra_param_selection)
            if extra_param_selection is not None:
                logging.info('Fitting discrete selection "%s".'
                             % extra_param_selection)

            # ignore alternate fits (it's complicated enough with the various
            # discrete hypo best fits we have already)
            this_hypo_fits, _ = self.fit_hypo(
                data_dist=data_dist,
                hypo_maker=hypo_maker,
                hypo_param_selections=full_param_selections,
                metric=metric,
                fit_settings=fit_settings,
                reset_free=reset_free,
                check_octant=check_octant,
                minimizer_settings=minimizer_settings,
                other_metrics=other_metrics,
                return_full_scan=return_full_scan,
                blind=blind,
                pprint=pprint,
            )

            fit_infos.append(this_hypo_fits)
            fit_metric_vals.append(
                [hypo_fit['metric_val'] for hypo_fit in this_hypo_fits]
            )
            fit_num_dists.append(
                [hypo_fit['num_distributions_generated'] for
                 hypo_fit in this_hypo_fits]
            )
            fit_times.append(
                [hypo_fit['fit_time'].m_as('second') for hypo_fit in this_hypo_fits]
            )
        # what's returned by fit_hypo can either be a full scan or just
        # a single point - in any case, for each point we now optimize
        # the extra selections manually
        if metric in METRICS_TO_MAXIMIZE:
            bf_dims = np.argmax(fit_metric_vals, axis=0)
        else:
            bf_dims = np.argmin(fit_metric_vals, axis=0)
        bf_num_dists = np.sum(fit_num_dists, axis=0)
        bf_fit_times = np.sum(fit_times, axis=0) * ureg.sec

        # select the fitting infos corresponding to these best metric values
        best_fit_infos = [fit_infos[dim][i] for i,dim in enumerate(bf_dims)]
        for num_dist, fit_time, bf_info in \
            zip(bf_num_dists, bf_fit_times, best_fit_infos):
            bf_info['num_distributions_generated'] = num_dist
            bf_info['fit_time'] = fit_time

        end_t = time.time()
        multi_hypo_fit_t = end_t - start_t

        if len(extra_param_selections) > 1:
            logging.info(
                'Total time to fit all discrete hypos: %8.4f s;'
                ' # of dists. generated: %6d',
                multi_hypo_fit_t, np.sum(bf_num_dists)
            )

        return best_fit_infos


    def optimize_t23_octant(self, best_fit_info, alternate_fits, data_dist,
                            hypo_maker, metric, minimizer_settings,
                            other_metrics, pprint, blind):
        # Hop to other octant by reflecting about 45 deg
        theta23 = hypo_maker.params.theta23
        inflection_point = (45*ureg.deg).to(theta23.units)
        theta23.value = 2*inflection_point - theta23.value
        hypo_maker.update_params(theta23)

        # Re-run minimizer starting at new point
        new_fit_info = self._fit_hypo_inner(
            hypo_maker=hypo_maker,
            data_dist=data_dist,
            metric=metric,
            minimizer_settings=minimizer_settings,
            other_metrics=other_metrics,
            pprint=pprint,
            blind=blind
        )

        # Check to make sure these two fits were either side of 45
        # degrees.
        old_octant = t23_octant(best_fit_info)
        new_octant = t23_octant(new_fit_info)

        if old_octant == new_octant:
            logging.warning(
                'Checking other octant was NOT successful since both '
                'fits have resulted in the same octant. Fit will be'
                ' tried again starting at a point further into '
                'the opposite octant.'
            )
            alternate_fits.append(new_fit_info)
            if old_octant > 0.0:
                theta23.value = min(
                    (55.0*ureg.deg).to(theta23.units),
                    max(theta23.range) - 0.01 * (max(theta23.range)-min(theta23.range))
                )
            else:
                theta23.value = max(
                    (35.0*ureg.deg).to(theta23.units),
                    min(theta23.range) + 0.01 * (max(theta23.range)-min(theta23.range))
                )
            hypo_maker.update_params(theta23)

            # Re-run minimizer starting at new point
            new_fit_info = self._fit_hypo_inner(
                hypo_maker=hypo_maker,
                data_dist=data_dist,
                metric=metric,
                minimizer_settings=minimizer_settings,
                other_metrics=other_metrics,
                pprint=pprint,
                blind=blind
            )
            # Make sure the new octant is sensible
            t23_octant(new_fit_info)

            if it_got_better(
                new_metric_val=new_fit_info['metric_val'],
                old_metric_val=best_fit_info['metric_val'],
                metric=metric
            ):
            # Take the one with the best fit
                alternate_fits.append(best_fit_info)
                best_fit_info = new_fit_info
                if not blind:
                    logging.debug('Accepting other-octant fit')
            else:
                alternate_fits.append(new_fit_info)
                if not blind:
                    logging.debug('Accepting initial-octant fit')

        return best_fit_info

    # TODO: fix docstring (not just here)
    def fit_hypo(self, data_dist, hypo_maker, hypo_param_selections, metric,
                 fit_settings=None, reset_free=True, check_octant=True,
                 minimizer_settings=None, other_metrics=None,
                 return_full_scan=False, blind=False, pprint=True):
        """Fitter "outer" loop: If `check_octant` is True, run
        `_fit_hypo_inner` starting in each octant of theta23 (assuming that
        is a param in the `hypo_maker`). Otherwise, just run the inner
        method once.

        Note that prior to running the fit, the `hypo_maker` has
        `hypo_param_selections` applied and its free parameters are reset to
        their nominal values.

        Parameters
        ----------
        data_dist : MapSet
            Data distribution(s). These are what the hypothesis is tasked to
            best describe during the optimization process.

        hypo_maker : DistributionMaker or instantiable thereto
            Generates the expectation distribution under a particular
            hypothesis. This typically has (but is not required to have) some
            free parameters which can be modified by the minimizer to optimize
            the `metric`.

        hypo_param_selections : None, string, or sequence of strings
            A pipeline configuration can have param selectors that allow
            switching a parameter among two or more values by specifying the
            corresponding param selector(s) here. This also allows for a single
            instance of a DistributionMaker to generate distributions from
            different hypotheses.

        metric : string
            The metric to use for optimization. Valid metrics are found in
            `VALID_METRICS`. Note that the optimized hypothesis also has this
            metric evaluated and reported for each of its output maps.

        minimizer_settings : string or dict

        check_octant : bool
            If theta23 is a parameter to be used in the optimization (i.e.,
            free), the fit will be re-run in the second (first) octant if
            theta23 is initialized in the first (second) octant.

        other_metrics : None, string, or list of strings
            After finding the best fit, these other metrics will be evaluated
            for each output that contributes to the overall fit. All strings
            must be valid metrics, as per `VALID_METRICS`, or the
            special string 'all' can be specified to evaluate all
            VALID_METRICS..

        pprint : bool
            Whether to show live-update of minimizer progress.

        blind : bool
            Whether to carry out a blind analysis. This hides actual parameter
            values from display and disallows these (as well as Jacobian,
            Hessian, etc.) from ending up in logfiles. It also resets the hypo
            maker's parameters to their nominal states, to prevent these from
            violating blindness when this method is run interactively.


        Returns
        -------
        best_fit_info : OrderedDict (see _fit_hypo_inner method for details of
            `fit_info` dict)
        alternate_fits : list of `fit_info` from other fits run

        """
        start_t = time.time()
        # set up lists for storing the fits
        best_fits = []
        alternate_fits = []

        # store the parameters to scan and the values to fix them to
        scan_params = []
        scan_vals = []

        # reset the counter whenever we start a new hypo fit
        self.counter = Counter()

        # Select the version of the parameters used for this hypothesis
        hypo_maker.select_params(hypo_param_selections)

        # only apply fit settings after the param selection has been applied
        if fit_settings is not None:
            fit_settings = apply_fit_settings(fit_settings, hypo_maker.params.free)

            minimize_params = fit_settings['minimize']['params']
            if minimize_params:
                # check if minimizer settings are passed into this method,
                # fall back to those given in fit settings
                if minimizer_settings is None:
                    # note: we assume these are parsed already!
                    minimizer_settings = {
                        'global': fit_settings['minimize']['global'],
                        'local': fit_settings['minimize']['local']
                    }
                else:
                    logging.warn(
                        'Minimizer settings provided as argument'
                        ' to `fit_hypo` used to override those in'
                        ' the fit settings!'
                    )
            else:
                if check_octant:
                    logging.warn(
                        'Selecting "check_octant" only useful if theta23'
                        ' is among *minimization* parameters. No need or no'
                        ' point with any other fitting method.'
                    )
                    check_octant = False

            scan_params = fit_settings['scan']['params']
            for i,pname in enumerate(scan_params):
                scan_vals.append([(pname, val) for val in
                                  fit_settings['scan']['values'][i]])
            fit_settings.pop('scan')
            # the parameters to scan over need to be fixed
            hypo_maker.params.fix(scan_params)

        else:
            # when there are no fit settings we want the default
            # behavior - just numerical minimization over all free
            # parameters: `_fit_hypo_inner` makes sure of this
            if hypo_maker.params.free and minimizer_settings is None:
                raise ValueError(
                    'You did not specify any fit settings, but there are free'
                    ' parameters which cannot be minimized over if there are'
                    ' no minimizer settings!'
                )

        # if there are no scan params/vals, we should inject the current
        # value for each parameter (do not require there to be free ones)
        if not scan_params:
            scan_vals = [[(pname, hypo_maker.params[pname].value)]
                          for pname in hypo_maker.params.names]

        # each scan point comes with its own best fit
        best_fit_ind = 0
        for i, pos in enumerate(product(*scan_vals)):
            msg = ''
            sep = ', '
            for (pname, val) in pos:
                hypo_maker.params[pname].value = val
                if scan_params and not blind:
                    if isinstance(val, float) or isinstance(val, ureg.Quantity):
                        if msg:
                            msg += sep
                        msg += '%s = %s'%(pname, val)
                    else:
                        raise TypeError("val is of type %s which I don't know "
                                        "how to deal with in the output "
                                        "messages."% type(val))
            if scan_params and not blind:
                # reporting the message below without the user having
                # requested a scan would be confusing/misleading
                logging.info('Fixing hypothesis parameters: %s.' % msg)
            # Reset free parameters to nominal values
            if reset_free:
                hypo_maker.reset_free()
            else:
                # Saves the current minimizer start values
                minimizer_start_params = hypo_maker.params

            best_fit_info = self._fit_hypo_inner(
                hypo_maker=hypo_maker,
                data_dist=data_dist,
                metric=metric,
                fit_settings_inner=fit_settings,
                minimizer_settings=minimizer_settings,
                other_metrics=other_metrics,
                pprint=pprint,
                blind=blind
            )

            if check_octant and 'theta23' in hypo_maker.params.free.names:
                if ('global' in minimizer_settings and
                    minimizer_settings['global'] is not None):
                    logging.info(
                        'Checking other octant of theta23 might not be'
                        ' necessary with a global minimizer. Doing so'
                        ' anyway right now.'
                    )
                logging.debug('checking other octant of theta23')
                if reset_free:
                    hypo_maker.reset_free()
                else:
                    for param in minimizer_start_params:
                        hypo_maker.params[param.name].value = param.value
                best_fit_info = self.optimize_t23_octant(
                    best_fit_info=best_fit_info,
                    alternate_fits=alternate_fits,
                    data_dist=data_dist,
                    hypo_maker=hypo_maker,
                    metric=metric,
                    minimizer_settings=minimizer_settings,
                    other_metrics=other_metrics,
                    pprint=pprint,
                    blind=blind
                )
            # make sure the overall best fit contains the
            # overall number of distributions generated
            # across the whole fitting process for this point
            best_fit_info['num_distributions_generated'] = self.counter.count
            # append the best fit for this scan point
            best_fits.append(best_fit_info)

            if i >= 1 and not return_full_scan:
                # in the case a scan was performed (more than a single point),
                # not returning the result of the full scan means we have
                # to determine which of the points corresponds to the global
                # best fit
                if it_got_better(
                       new_metric_val=best_fit_info['metric_val'],
                       old_metric_val=best_fits[i-1]['metric_val'],
                       metric=metric
                    ):
                    best_fit_ind = i

        end_t = time.time()
        fit_t = end_t - start_t

        logging.info(
            'Total time to fit hypo: %8.4f s;'
            ' # of dists generated: %6d',
            fit_t, self.counter.count,
        )

        # always return full list of alternate_fits for now
        if not return_full_scan:
            # interpret all points scanned as part of the fitting process
            best_fits[best_fit_ind]['num_distributions_generated'] =\
                self.counter.count
            best_fits[best_fit_ind]['fit_time'] = fit_t * ureg.sec
            # only return best fitting point
            return [best_fits[best_fit_ind]], alternate_fits

        return best_fits, alternate_fits


    def _fit_hypo_inner(self, data_dist, hypo_maker, metric,
                       fit_settings_inner=None, minimizer_settings=None,
                       other_metrics=None, pprint=True, blind=False):

        if fit_settings_inner is not None:
            pull_params = fit_settings_inner['pull']['params']
            minimize_params = fit_settings_inner['minimize']['params']
        else:
            # the default: just minimizer over all free
            pull_params = []
            minimize_params = hypo_maker.params.free.names

        # dispatch correct fitting method depending on combination of
        # pull and minimize params

        # no parameters to fit
        if not len(pull_params) and not len(minimize_params):
            logging.debug("Nothing else to do. Calculating metric(s).")
            nofit_hypo_asimov_dist = hypo_maker.get_outputs(return_sum=True)
            self.counter += 1
            fit_info = self.nofit_hypo(
                data_dist=data_dist,
                hypo_params=hypo_maker.params,
                hypo_asimov_dist=nofit_hypo_asimov_dist,
                metric=metric,
                other_metrics=other_metrics,
                blind=blind
           )

        # only parameters to optimize numerically
        elif len(minimize_params) and not len(pull_params):
            fit_info = self.fit_hypo_minimizer(
                data_dist=data_dist,
                hypo_maker=hypo_maker,
                minimizer_settings=minimizer_settings,
                metric=metric,
                other_metrics=other_metrics,
                blind=blind,
                pprint=pprint
            )

        # only parameters to fit with pull method
        elif len(pull_params) and not len(minimize_params):
            fit_info = self.fit_hypo_pull(
                data_dist=data_dist,
                hypo_maker=hypo_maker,
                pull_settings=fit_settings_inner['pull'],
                metric=metric,
                other_metrics=other_metrics,
                blind=blind
            )
        # parameters to optimize numerically and to fit with pull method
        else:
            raise NotImplementedError(
                "Combination of minimization and pull method not implemented yet!"
            )
        return fit_info


    def fit_hypo_minimizer(self, data_dist, hypo_maker, metric, minimizer_settings,
                           other_metrics=None, pprint=True, blind=False):
        """Fitter "inner" loop: Run an arbitrary scipy minimizer to modify
        hypo dist maker's free params until the data_dist is most likely to have
        come from this hypothesis.

        Note that an "outer" loop can handle discrete scanning over e.g. the
        octant for theta23; for each discrete point the "outer" loop can make a
        call to this "inner" loop. One such "outer" loop is implemented in the
        `fit_hypo` method.


        Parameters
        ----------
        data_dist : MapSet
            Data distribution(s)

        hypo_maker : DistributionMaker or convertible thereto

        metric : string

        minimizer_settings : dict

        other_metrics : None, string, or sequence of strings

        pprint : bool
            Whether to show live-update of minimizer progress.

        blind : bool


        Returns
        -------
        fit_info : OrderedDict with details of the fit with keys 'metric',
            'metric_val', 'params', 'hypo_asimov_dist', and
            'minimizer_metadata'

        """
        if set(minimizer_settings.keys()) == set(('local', 'global')):
            # allow for an entry of `None`
            for minimizer_type in ['local', 'global']:
                try:
                    minimizer_type_settings =\
                        set_minimizer_defaults(minimizer_settings[minimizer_type])
                    validate_minimizer_settings(minimizer_type_settings)
                except:
                    minimizer_type_settings = None
                minimizer_settings[minimizer_type] = minimizer_type_settings
        else:
            # just try to interpret as "regular" local minimization
            method = minimizer_settings['method'].lower()
            if not method in LOCAL_MINIMIZERS_WITH_DEFAULTS:
                raise ValueError(
                    'Minimizer method "%s" could not be identified as'
                    ' corresponding to local minimization (valid methods: %s).'
                    ' If you desire to run a global minimizer pass in the'
                    ' config with explicit "global" and "local" keys.'
                    % (method, LOCAL_MINIMIZERS_WITH_DEFAULTS)
                )
            minimizer_settings = set_minimizer_defaults(minimizer_settings)
            validate_minimizer_settings(minimizer_settings)
            new_minimizer_settings = {
                'global': None,
                'local': minimizer_settings
            }
            minimizer_settings = new_minimizer_settings

        sign = -1 if metric in METRICS_TO_MAXIMIZE else +1

        # set starting values and bounds (bounds possibly modified depending
        # on whether the local minimizer uses gradients)
        x0, bounds = minimizer_x0_bounds(
            free_params=hypo_maker.params.free,
            minimizer_settings=minimizer_settings['local']
        )

        fit_history = []
        fit_history.append([metric] + [p.name for p in hypo_maker.params.free])

        if pprint and not blind:
            # display header if desired/allowed
            display_minimizer_header(
                free_params=hypo_maker.params.free,
                metric=metric
            )

        # reset number of iterations before each minimization
        self._nit = 0
        # also create a dedicated counter for this one
        # minimization process
        min_counter = Counter()

        # record start time
        start_t = time.time()

        # this is the function that does the heavy lifting
        optimize_result = run_minimizer(
            fun=self._minimizer_callable,
            x0=x0,
            bounds=bounds,
            minimizer_settings=minimizer_settings,
            minimizer_callback=self._minimizer_callback,
            hypo_maker=hypo_maker,
            data_dist=data_dist,
            metric=metric,
            counter=min_counter,
            fit_history=fit_history,
            pprint=pprint,
            blind=blind
        )

        if pprint:
            # clear the line
            sys.stdout.write('\n\n')
            sys.stdout.flush()

        # Will not assume that the minimizer left the hypo maker in the
        # minimized state, so set the values now (also does conversion of
        # values from [0,1] back to physical range)
        rescaled_pvals = optimize_result.pop('x')
        hypo_maker._set_rescaled_free_params(rescaled_pvals) # pylint: disable=protected-access

        # Record the Asimov distribution with the optimal param values
        hypo_asimov_dist = hypo_maker.get_outputs(return_sum=True)
        min_counter += 1

        # update the global counter
        self.counter += min_counter.count

        # Get the best-fit metric value
        metric_val = sign * optimize_result.pop('fun')

        end_t = time.time()
        minimizer_time = end_t - start_t

        logging.info(
            'Total time to minimize: %8.4f s;'
            ' # of dists. generated: %6d;'
            ' avg. dist. gen. time: %10.4f ms',
            minimizer_time, min_counter.count,
            minimizer_time*1000./min_counter.count
        )

        # Record minimizer metadata (all info besides 'x' and 'fun'; also do
        # not record some attributes if performing blinded analysis)
        metadata = OrderedDict()
        for k in sorted(optimize_result.keys()):
            if blind and k in ['jac', 'hess', 'hess_inv']:
                continue
            metadata[k] = optimize_result[k]

        fit_info = OrderedDict()
        fit_info['metric'] = metric
        fit_info['metric_val'] = metric_val
        if blind:
            hypo_maker.reset_free()
            fit_info['params'] = ParamSet()
        else:
            fit_info['params'] = deepcopy(hypo_maker.params)
        fit_info['detailed_metric_info'] = self.get_detailed_metric_info(
            data_dist=data_dist, hypo_asimov_dist=hypo_asimov_dist,
            params=hypo_maker.params, metric=metric, other_metrics=other_metrics,
            blind=blind
        )
        fit_info['fit_time'] = minimizer_time * ureg.sec
        # store the no. of distributions for this minimization process
        fit_info['num_distributions_generated'] = min_counter.count
        fit_info['fit_metadata'] = metadata
        fit_info['fit_history'] = fit_history
        # If blind replace hypo_asimov_dist with none object
        if blind:
            hypo_asimov_dist = None
        fit_info['hypo_asimov_dist'] = hypo_asimov_dist

        msg = optimize_result.message
        if blind:
            msg = ''

        if hasattr(optimize_result, 'success'):
            if not optimize_result.success:
                raise ValueError('Optimization failed. Message: "%s"' % msg)
        else:
            logging.warn('Could not tell whether optimization was successful -'
                         ' most likely because global optimization was'
                         ' requested. Message: "%s"' % msg)

        return fit_info


    def fit_hypo_pull(self, data_dist, hypo_maker, pull_settings, metric,
                      other_metrics=None, pprint=True, blind=False):
        """Fit a hypo to a data distribution via the pull method.

        Returns
        -------
        fit_info : OrderedDict with details of the fit with keys 'metric',
            'metric_val', 'params', 'hypo_asimov_dist'
        """
        fit_info = OrderedDict()
        fit_info['metric'] = metric

        # currently only chi2 fit implemented
        assert metric == 'chi2'

        # record start time
        start_t = time.time()

        pull_counter = Counter()

        # main algorithm: calculate fisher matrix and parameter pulls
        # TODO: check this is indeed generated at the fiducial model
        test_vals = {pname: pull_settings['values'][i] for i,pname in
                     enumerate(pull_settings['params'])}

        fisher, gradient_maps, fid_hypo_asimov_dist = get_fisher_matrix(
            data_dist=data_dist,
            hypo_maker=hypo_maker,
            test_vals=test_vals,
            counter=pull_counter
        )

        pulls = calculate_pulls(
            fisher=fisher,
            fid_maps_truth=data_dist,
            fid_hypo_asimov_dist=fid_hypo_asimov_dist,
            gradient_maps=gradient_maps,
        )
        #fit_params = deepcopy(hypo_maker.params)

        # update hypo maker params to best fit values
        for pname, pull in pulls:
            hypo_maker.params[pname].value = (
                hypo_maker.params[pname].nominal_value + pull
            )

        # generate the hypo distribution at the best fit
        best_fit_hypo_dist = hypo_maker.get_outputs(return_sum=True)
        pull_counter += 1
        self.counter += pull_counter.count

        # calculate the value of the metric at the best fit
        metric_val = (
                data_dist.metric_total(expected_values=best_fit_hypo_dist,
                                       metric=metric)
                + hypo_maker.params.priors_penalty(metric=metric)
        )

        # record stop time
        end_t = time.time()

        # store the metric value
        fit_info['metric_val'] = metric_val

        # store the fit duration
        fit_t = end_t - start_t

        logging.info(
            'Total time to compute pulls: %8.4f s;'
            ' # of dists. generated: %6d',
            fit_t, pull_counter.count,
        )

        fit_info['fit_time'] = fit_t * ureg.sec

        if blind:
            hypo_maker.reset_free()
            fit_info['params'] = ParamSet()
        else:
            fit_info['params'] = hypo_maker.params
        fit_info['detailed_metric_info'] = self.get_detailed_metric_info(
            data_dist=data_dist, hypo_asimov_dist=best_fit_hypo_dist,
            params=hypo_maker.params, metric=metric, other_metrics=other_metrics,
            blind=blind
        )
        fit_info['num_distributions_generated'] = pull_counter.count
        if blind:
            best_fit_hypo_dist = None
        fit_info['hypo_asimov_dist'] = best_fit_hypo_dist

        return fit_info


    def nofit_hypo(self, data_dist, hypo_params, hypo_asimov_dist, metric,
                   other_metrics=None, blind=False):
        """Fitting a hypo to Asimov distribution generated by its own
        distribution maker is unnecessary. In such a case, use this method
        (instead of `fit_hypo`) to still retrieve meaningful information for
        e.g. the match metrics.

        Parameters
        ----------
        data_dist : MapSet
        hypo_maker : DistributionMaker
        hypo_param_selections : None, string, or sequence of strings
        hypo_asimov_dist : MapSet
        metric : string
        other_metrics : None, string, or sequence of strings
        blind : bool

        """
        fit_info = OrderedDict()
        fit_info['metric'] = metric

        # record start time
        start_t = time.time()

        # Assess the fit: whether the data came from the hypo_asimov_dist
        try:
            metric_val = (
                data_dist.metric_total(expected_values=hypo_asimov_dist,
                                       metric=metric)
                + hypo_params.priors_penalty(metric=metric)
            )
        except:
            if not blind:
                logging.error(
                    'Failed when computing metric with params %s.', hypo_params
                )
            raise

        # record stop time
        end_t = time.time()
        # store the "fit" duration
        fit_t = end_t - start_t

        fit_info['metric_val'] = metric_val

        if blind:
            fit_info['params'] = ParamSet()
        else:
            fit_info['params'] = deepcopy(hypo_params)
        fit_info['detailed_metric_info'] = self.get_detailed_metric_info(
            data_dist=data_dist, hypo_asimov_dist=hypo_asimov_dist,
            params=hypo_params, metric=metric, other_metrics=other_metrics,
            blind=blind
        )
        fit_info['fit_time'] = fit_t * ureg.sec
        fit_info['num_distributions_generated'] = 1
        fit_info['fit_metadata'] = OrderedDict()
        # If blind replace hypo_asimov_dist with none object
        if blind:
            hypo_asimov_dist = None
        fit_info['hypo_asimov_dist'] = hypo_asimov_dist
        return fit_info

    @staticmethod
    def get_detailed_metric_info(data_dist, hypo_asimov_dist, params, metric,
                                 other_metrics=None, blind=False):
        """Get detailed fit information, including e.g. maps that yielded the
        metric.

        Parameters
        ----------
        data_dist
        hypo_asimov_dist
        params
        metric
        other_metrics
        blind

        Returns
        -------
        detailed_metric_info : OrderedDict

        """
        if other_metrics is None:
            other_metrics = []
        elif isinstance(other_metrics, basestring):
            other_metrics = [other_metrics]
        all_metrics = sorted(set([metric] + other_metrics))
        detailed_metric_info = OrderedDict()
        for m in all_metrics:
            name_vals_d = OrderedDict()
            name_vals_d['maps'] = data_dist.metric_per_map(
                expected_values=hypo_asimov_dist, metric=m
            )
            metric_hists = data_dist.metric_per_map(
                expected_values=hypo_asimov_dist, metric='binned_'+m
            )
            maps_binned = []
            for asimov_map, metric_hist in zip(hypo_asimov_dist, metric_hists):
                map_binned = Map(
                    name=asimov_map.name,
                    hist=np.reshape(metric_hists[metric_hist],
                                    asimov_map.shape),
                    binning=asimov_map.binning
                )
                maps_binned.append(map_binned)
            name_vals_d['maps_binned'] = MapSet(maps_binned)
            # do not record param priors in case of blind analysis
            priors_penalties = (
                params.priors_penalties(metric=metric) if not blind else None
            )
            name_vals_d['priors'] = priors_penalties
            detailed_metric_info[m] = name_vals_d
        return detailed_metric_info

    def _minimizer_callable(self, scaled_param_vals, hypo_maker, data_dist,
                            metric, counter, fit_history, pprint, blind):
        """Simple callback for use by scipy.optimize minimizers.

        This should *not* in general be called by users, as `scaled_param_vals`
        are stripped of their units and scaled to the range [0, 1], and hence
        some validation of inputs is bypassed by this method.

        Parameters
        ----------
        scaled_param_vals : sequence of floats
            If called from a scipy.optimize minimizer, this sequence is
            provieded by the minimizer itself. These values are all expected to
            be in the range [0, 1] and be simple floats (no units or
            uncertainties attached, etc.). Rescaling the parameter values to
            their original (physical) ranges (including units) is handled
            within this method.

        hypo_maker : DistributionMaker
            Creates the per-bin expectation values per map (aka Asimov
            distribution) based on its param values. Free params in the
            `hypo_maker` are modified by the minimizer to achieve a "best" fit.

        data_dist : MapSet
            Data distribution to be fit. Can be an actual-, Asimov-, or
            pseudo-data distribution (where the latter two are derived from
            simulation and so aren't technically "data").

        metric : str
            Metric by which to evaluate the fit. See Map

        counter : Counter
            Mutable object to keep track--outside this method--of the number of
            times this method is called.

        pprint : bool
            Displays a single-line that updates live (assuming the entire line
            fits the width of your TTY).

        blind : bool

        """
        # Want to *maximize* e.g. log-likelihood but we're using a minimizer,
        # so flip sign of metric in those cases.
        sign = -1 if metric in METRICS_TO_MAXIMIZE else +1

        # Set param values from the scaled versions the minimizer works with
        hypo_maker._set_rescaled_free_params(scaled_param_vals) # pylint: disable=protected-access

        # Get the Asimov map set
        try:
            hypo_asimov_dist = hypo_maker.get_outputs(return_sum=True)
        except:
            if not blind:
                logging.error(
                    'Failed to generate Asimov distribution with free'
                    ' params %s', hypo_maker.params.free
                )
            raise

        # Assess the fit: whether the data came from the hypo_asimov_dist
        try:
            metric_val = (
                data_dist.metric_total(expected_values=hypo_asimov_dist,
                                       metric=metric)
                + hypo_maker.params.priors_penalty(metric=metric)
            )
        except:
            if not blind:
                logging.error(
                    'Failed when computing metric with free params %s',
                    hypo_maker.params.free
                )
            raise

        # Report status of metric & params (except if blinded)
        if blind:
            msg = ('minimizer iteration: #%6d | function call: #%6d'
                   %(self._nit, counter.count))
        else:
            #msg = '%s=%.6e | %s' %(metric, metric_val, hypo_maker.params.free)
            msg = '%s %s %s | ' %(('%d'%self._nit).center(6),
                                  ('%d'%counter.count).center(10),
                                  format(metric_val, '0.5e').rjust(12))
            msg += ' '.join([('%0.5e'%p.value.m).rjust(12)
                             for p in hypo_maker.params.free])

        if pprint:
            sys.stdout.write(msg)
            sys.stdout.flush()
            sys.stdout.write('\b' * len(msg))
        else:
            logging.trace(msg)

        counter += 1

        if not blind:
            fit_history.append(
                [metric_val] + [v.value.m for v in hypo_maker.params.free]
            )

        return sign*metric_val

    def _minimizer_callback(self, xk): # pylint: disable=unused-argument
        """Passed as `callback` parameter to `optimize.minimize`, and is called
        after each iteration. Keeps track of number of iterations.

        Parameters
        ----------
        xk : list
            Parameter vector

        """
        self._nit += 1


def test_fitting():
    """Testing. Could easily break because heavily relies on external stuff."""
    from pisa.core import distribution_maker
    from pisa.utils.log import set_verbosity
    from pisa.utils.resources import find_resource
    set_verbosity(1)
    ana = Analysis()
    example_directory = find_resource('settings/pipeline')
    example_pipeline = example_directory + '/example_param_discrete_hypos.cfg'

    d = distribution_maker.DistributionMaker(pipelines=example_pipeline)
    data_dist = d.get_outputs(return_sum=True)

    fix = ('nu_nubar_ratio', 'energy_scale', 'nu_nubar_ratio', 'nue_numu_ratio',
           'theta13', 'aeff_scale', 'atm_delta_index')
    for pname in fix:
        try:
            d.params[pname].is_fixed = True
        except:
            pass

    if not 'theta23' in d.params.free.names or not 'deltam31' in d.params.free.names:
        return

    d.params.theta23.prior = None
    d.params.deltam31.prior = None
    free_params = d.params.free

    # define some fit settings for a scan first (without any minimization)
    fit_settings = {'scan': {
                        'params': {
                            'deltam31': {
                                'values': 'np.linspace(2.4e-3, 2.6e-3, 10) * units.eV**2'
                            }, # we can either give values directly or a range
                               # around nominal + a number of lin. spaced values
                               # (useful e.g. for different discrete hypos)
                            'theta23': {
                                'nvalues': 10, 'range': 'nominal+/-0.2*nominal'
                            },
                        },
                    },
                    'minimize': {'params': {}},
                    'pull': {'params': {}},
    }
    fit_info, _ = ana.fit_hypo(
        data_dist=data_dist,
        hypo_maker=d,
        hypo_param_selections=[None],
        metric='chi2',
        fit_settings=fit_settings,
        reset_free=True,
        check_octant=False,
        minimizer_settings=None,
        other_metrics=None,
        return_full_scan=False,
        blind=False,
        pprint=True
    )

    logging.info('Value of metric at best fit (scan): %s'
                 % fit_info[0]['metric_val'])

    # redefine d because it lost its free scan params
    d = distribution_maker.DistributionMaker(pipelines=example_pipeline)
    # let's generate the distribution for theta23 off-nominal (to test whether
    # the global optimizer can recover the value for the matching discrete hypo)
    d.params['theta23'].value = 49.7 * ureg.degree
    data_dist = d.get_outputs(return_sum=True)
    d.reset_free()

    fix = ('nu_nubar_ratio', 'energy_scale', 'nu_nubar_ratio', 'nue_numu_ratio',
           'theta13', 'aeff_scale', 'atm_delta_index')
    for pname in fix:
        try:
            d.params[pname].is_fixed = True
        except:
            pass

    if not 'theta23' in d.params.free.names or not 'deltam31' in d.params.free.names:
        return

    d.params.theta23.prior = None
    d.params.deltam31.prior = None
    free_params = d.params.free

    # global minimization over discrete hypotheses
    # setup the minimizer settings
    minimizer_cfg_directory = find_resource('settings/minimizer')
    minimizer_settings_global = parse_minimizer_config(
         minimizer_cfg_directory +
        '/basinhopping_niter100_niter_success2_T1e0_stepsize5e-1_interval50.cfg'
    )
    minimizer_settings_local = parse_minimizer_config(
        minimizer_cfg_directory +
        '/l-bfgs-b_ftol2e-9_gtol1e-5_eps1e-7_maxiter200.cfg'
    )

    minimizer_settings = {}
    minimizer_settings['global'] = minimizer_settings_global
    minimizer_settings['local'] = minimizer_settings_local

    fit_settings = {'scan': {'params': {}},
                    'minimize': {'params': {'*': {}},
                                 'defaults': minimizer_settings},
                    'pull': {'params': {}}
                   }

    fit_info = ana.optimize_discrete_selections(
        data_dist=data_dist,
        hypo_maker=d,
        hypo_param_selections=[None],
        extra_param_selections=['wrong1', 'wrong2', 'correct', 'wrong3'],
        metric='chi2',
        fit_settings=fit_settings, # equivalent to not using any for min. only
        reset_free=True,
        check_octant=False,
        minimizer_settings=minimizer_settings,
        other_metrics=None,
        return_full_scan=False,
        blind=False,
        pprint=True
    )

    logging.info('Value of metric at best fit: %s' % fit_info[0]['metric_val'])

if __name__ == '__main__':
    test_fitting()
